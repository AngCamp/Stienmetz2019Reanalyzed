{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee7bcc4-115a-4a1c-85f4-90a71886bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "from distutils.command.build_scripts import first_line_re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from math import floor\n",
    "import scipy.ndimage\n",
    "import timeit #for testing and tracking run times\n",
    "import scipy.stats \n",
    "import getSteinmetz2019data as stein\n",
    "import warnings\n",
    "import sklearn\n",
    "\n",
    "# %%\n",
    "import KernelRegDraft as kreg\n",
    "import piso\n",
    "\n",
    "# notes:  look if SWR tuned cells are more active in no-go error trials and low contrast error trials espescially those with \n",
    "# a high bias of previously rewarded trials \n",
    "\n",
    "#setting path to the data\n",
    "path_to_data = os.path.relpath('/mnt/c/Users/angus/Desktop/SteinmetzLab/9598406/spikeAndBehavioralData/allData/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a754466f-63ae-494c-9390-b9ec8135dedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/users/angus/Desktop/SteinmetzLab/Analysis'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7753623-3df5-49dd-9a40-d177b573fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting path to the data\n",
    "path_to_data = os.path.relpath('/mnt/c/Users/angus/Desktop/SteinmetzLab/9598406/spikeAndBehavioralData/allData/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7073df1d-b3a2-41c6-808f-dd33293c01d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016425999929197133\n"
     ]
    }
   ],
   "source": [
    "start = timeit.timeit()\n",
    "#These trials selected because they contain all types of choices, left 2 rights then a no go\n",
    "# [4,5,6,7]\n",
    "\n",
    "#test this fucntion out\n",
    "#note steinmetz mthods uses P and X interchanably so\n",
    "# I thought ti would be appropriate here\n",
    "\n",
    "P = kreg.make_toeplitz_matrix(session = 'Theiler_2017-10-11', \n",
    "                     bin_size = 0.005, \n",
    "                     kernels = [True, True, True],\n",
    "                     select_trials=np.array([4,5,6,7]),\n",
    "                     filepath = path_to_data\n",
    "                             )\n",
    "\n",
    "\n",
    "end= timeit.timeit()\n",
    "print(start-end)\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "734da320-36b7-4340-9080-7ecfc719400b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0005496999947354198\n"
     ]
    }
   ],
   "source": [
    "start = timeit.timeit()\n",
    "P = kreg.make_toeplitz_matrix(session = 'Theiler_2017-10-11', \n",
    "                     bin_size = 0.005, \n",
    "                     kernels = [True, True, True],\n",
    "                     select_trials=np.array([4,5,6,7]),\n",
    "                     filepath = path_to_data)\n",
    "end= timeit.timeit()\n",
    "print(start-end)\n",
    "\n",
    "# note prehaps make the kernels a seperate funciton which you then supply to make_toeplitz_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e928fd-e488-4436-969b-57185b94aaba",
   "metadata": {},
   "source": [
    "<br>\n",
    "So we want to include a kernel for the SWR in various parts of the hippocampus.  There is a long held consensuss in the literature that SWRs are typically generated in the CA3-CA2 network (Csicsvari et al., 2000) with CA2 potentially being more invovled in waking SWR generation.  Other regions may serve to generate them as well though these event seem rarer, less well cahracterized and would be described as atypical (Imbrosci et al., 2021) so for now they will not be a focus of this analysis.  SWRs serve as a marker of activation of memory during rest to guide upcoming behaviour as such we will try to avoid detecting movement associated rhythms and restrict our analyses to periods of rest and immobility defined as outside of +/- 50ms of a flinch or wheel turn interval.  Time within and outside of trial intervals will be included as we are not trying to infer the congnitive relevance of the neurons simply if their activity is predicting or predicted by a SWR event.\n",
    "<br>\n",
    "Ref:\n",
    "<br>\n",
    "Csicsvari, J., Hirase, H., Mamiya, A., & Buzsáki, G. (2000). Ensemble patterns of hippocampal CA3-CA1 neurons during sharp wave–associated population events. Neuron, 28(2), 585-594.\n",
    "<br>\n",
    "Imbrosci, B., Nitzan, N., McKenzie, S., Donoso, J. R., Swaminathan, A., Böhm, C., ... & Schmitz, D. (2021). Subiculum as a generator of sharp wave-ripples in the rodent hippocampus. Cell reports, 35(3), 109021.\n",
    "<br>\n",
    "Oliva, Azahara, Antonio Fernández-Ruiz, György Buzsáki, and Antal Berényi. \"Role of hippocampal CA2 region in triggering sharp-wave ripples.\" Neuron 91, no. 6 (2016): 1342-1355.\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9eab491-8688-4dac-8967-6acb01f3d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#angus_localpath = os.fspath(r'C:\\Users\\angus\\Desktop\\SteinmetzLab\\9598406\\spikeAndBehavioralData\\allData')\n",
    "import KernelRegDraft as kreg\n",
    "fetched_obj = stein.calldata('Tatum_2017-12-09',\n",
    "                                    ['wheelMoves.intervals.npy',\n",
    "                                    'licks.times.npy'],\n",
    "                                    steinmetzpath = path_to_data)\n",
    "\n",
    "movesintervals = fetched_obj['wheelMovesintervals']\n",
    "lickstimes = fetched_obj['lickstimes']\n",
    "\n",
    "\n",
    "\n",
    "licksintervals = kreg.generate_event_interval(lickstimes, [-0.025,0.025])\n",
    "movement_intervals = kreg.combine_intervals(licksintervals, movesintervals)\n",
    "#now to insert this into your make_freq array, started in freq_array_v2 so as to accept a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67b8f9e7-1579-4a0c-badb-addf8ed84a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  32.49316109,   32.54316109],\n",
       "       [  32.60116308,   32.65116308],\n",
       "       [  32.67916452,   32.72916452],\n",
       "       ...,\n",
       "       [2934.03716053, 2934.15716053],\n",
       "       [2935.27716053, 2935.46916053],\n",
       "       [2973.22016053, 2973.34116053]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movement_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc692ee-58a0-4503-b735-2e1b2fa38f2d",
   "metadata": {},
   "source": [
    "Next we want..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b40af6b-c905-498c-b76a-561ded3715a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['channelsbrainLocation', 'channelsprobe'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetched_obj = stein.calldata('Tatum_2017-12-09',\n",
    "                             ['channels.brainLocation.tsv','channels.probe.npy'],\n",
    "                             steinmetzpath= path_to_data, \n",
    "                             propertysearch = False)\n",
    "\n",
    "fetched_obj.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "327cf6e9-98ec-4b8b-976a-ee57ed506bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the df is the expected shape...\n",
      "(748, 5)\n",
      "\n",
      " Number of channels in each location... \n",
      "root    392\n",
      "GPe     106\n",
      "BLA     104\n",
      "LGd      66\n",
      "SNr      46\n",
      "CA3      34\n",
      "Name: allen_ontology, dtype: int64\n",
      "\n",
      " Number of channels in CA3 and from which probe... \n",
      "1.0    34\n",
      "Name: probe, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "chan_locations = fetched_obj['channelsbrainLocation']\n",
    "chan_locations['probe'] = fetched_obj['channelsprobe']\n",
    "locations = pd.Series(chan_locations['allen_ontology'])\n",
    "\n",
    "print(\"Checking the df is the expected shape...\")\n",
    "print(chan_locations.shape)\n",
    "print()\n",
    "print(\" Number of channels in each location... \")\n",
    "print( chan_locations['allen_ontology'].value_counts() )\n",
    "print()\n",
    "print(\" Number of channels in CA3 and from which probe... \")\n",
    "print( chan_locations.loc[ chan_locations['allen_ontology'] == 'CA3'  , 'probe'].value_counts()  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd316c5-419c-4f4d-b54f-cf6c8c228524",
   "metadata": {},
   "source": [
    "So we have 34 channels in CA3 in this recording, conveniently we also have GPe and SNr which may be of interest later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946563c0-2f2e-4d7b-abbe-8fbe3577442a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ccf_ap', 'ccf_dv', 'ccf_lr', 'allen_ontology', 'probe'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chan_locations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "262c72ce-ab55-4cf7-8c4f-c80955bd0bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    374\n",
       "1.0    374\n",
       "Name: probe, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chan_locations.probe.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b952d7e6-b4f3-4927-b1ff-7cb4cb58ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = chan_locations.groupby(chan_locations.probe)\n",
    "\n",
    "probe2 = grouped.get_group(1.0)\n",
    "probe2 = probe2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fb5bf7c-c857-43b4-ac28-6ec3f423932a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe2.index[probe2.allen_ontology == 'CA3'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8142b2-328e-4a3f-84d1-1392a6ba9c85",
   "metadata": {},
   "source": [
    "There are drifts in the clocks, see the description below from the Steinmetz data.  This means the timestamps must be used to \n",
    "correct for this.  The following is from the LFP data on figshare: https://figshare.com/articles/dataset/LFP_data_from_Steinmetz_et_al_2019/9727895 \n",
    "\n",
    "*\"Each file has different start and end times, and there are drifts in the clocks between probes - \n",
    "so the timestamp files, which give the aligned timing information, must be used. \n",
    "These files are located in the companion dataset (with spiking and behavioral data).\n",
    " The aligned timestamps  are specified in a particular way: an Nx2 matrix where the first column \n",
    " is sample numbers and the second column is the time in seconds at which those samples occurred. \n",
    " Since LFP was sampled regularly, N=2, just the first and last sample. So, to get the time at \n",
    " which every sample occurred, use this line in matlab:*\n",
    "\n",
    "tsData = readNPY('...lf.timestamps.npy');\n",
    "allTS = interp1(tsData(:,1), tsData(:,2), tsData(1,1):tsData(2,1));\n",
    "\n",
    " *That's just linearly interpolating between the times given for the first and last samples.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc5d0e67-2d61-44c0-a435-c3c46b6a6791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Tatum_20171209_K3_g0_t0imeclftimestamps', 'Tatum_20171209_K1_g0_t0imeclftimestamps'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.38937876e+00],\n",
       "       [7.45897100e+06, 2.98499185e+03]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetched_obj = stein.calldata('Tatum_2017-12-09',\n",
    "                            ['imec.lf.timestamps.'],\n",
    "                             propertysearch = True,\n",
    "                            steinmetzpath = path_to_data)\n",
    "\n",
    "\n",
    "fetched_obj = fetched_obj['imec.lf.timestamps']\n",
    "print( fetched_obj.keys() )\n",
    "timestamps_k1 = fetched_obj['Tatum_20171209_K3_g0_t0imeclftimestamps']\n",
    "timestamps_k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c63d072c-5997-4422-84e2-6ea24a17275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88e3c228-b49f-47ab-a4af-828e1535bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_file = 'Tatum_2017-12-09_K1_g0_t0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fea94cb-f9af-4dbd-9adc-e93194320b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open(\"probefile.csv\", \"w\")\n",
    "n = csv_file.write(probe_file)\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54bede2e-e0e1-41c8-a97d-d9aebb5099f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (1027464298.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [23]\u001b[0;36m\u001b[0m\n\u001b[0;31m    'C:\\Users\\angus\\Desktop\\SteinmetzLab\\LFP\\Tatum_2017-12-09_K1_g0_t0.imec.lf.bin')\u001b[0m\n\u001b[0m                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "#write the filepath to the timestamps into a csv\n",
    "#save the movement intervals as an NPY \n",
    "\n",
    "#requires import csv, its a base python library\n",
    "\n",
    "def shortwaveripple_kernel(intervals_of_interest, \n",
    "                           probe_timestamps,\n",
    "                           probe_file):\n",
    "    \"\"\"Makes a kernel of the SWRs occuring in intervals of interest,\n",
    "    indended to be used when animal is not turning the wheel or flinching.\n",
    "    \n",
    "    intervals_of_interest: an intervals obeject either from the user or from the \n",
    "    probe_timestamps_of_interest: time stamps for the associated probe\n",
    "    probe_file: filename string for the file\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    np.save('these_intervals.npy', interval_of_interest)\n",
    "    np.save('this_timestamps', timestamps_k1)\n",
    "    csv_file = open(\"probefile.csv\", \"w\")\n",
    "    n = csv_file.write(probe_file)\n",
    "    csv_file.close()\n",
    "    \n",
    "    #eventually we will activate and call the SWR detector from matlab\n",
    "    #matlab engine on\n",
    "    #rippledetector.m\n",
    "    #matlab engine off\n",
    "    \n",
    "    #remove the files we created to pass to matlab\n",
    "    os.remove('these_interval.npy')\n",
    "    os.remove('this_timestamp.npy')\n",
    "    os.remove('probefile.txt')\n",
    "    #return(SWR_kernel.npy)\n",
    "\n",
    "\n",
    "\n",
    "shortwaveripple_kernel(movement_intervals, timestamps_k1, \n",
    "                       'C:\\Users\\angus\\Desktop\\SteinmetzLab\\LFP\\Tatum_2017-12-09_K1_g0_t0.imec.lf.bin')\n",
    "\n",
    "#delete these files after running the matlab script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b953d16-46f0-4524-81b5-99728fe0b180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00466667],\n",
       "       [0.00546667],\n",
       "       [0.00626667],\n",
       "       [0.00656667],\n",
       "       [0.00676667],\n",
       "       [0.0069    ],\n",
       "       [0.00796667],\n",
       "       [0.0086    ],\n",
       "       [0.0091    ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetched_obj = stein.calldata('Tatum_2017-12-09',\n",
    "                                    ['spikes.times.npy'],\n",
    "                                    steinmetzpath = path_to_data)\n",
    "\n",
    "spiketimes = fetched_obj['spikestimes']\n",
    "spiketimes[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838b0ce-5a05-4ee3-b4b0-b4c7e15ad61f",
   "metadata": {},
   "source": [
    "Calling matlab from python so we can use steinmetz's scripts.  We can use the matlab.engine\n",
    "\n",
    "\n",
    "https://www.mathworks.com/help/matlab/matlab-engine-for-python.html\n",
    "https://www.mathworks.com/content/dam/mathworks/mathworks-dot-com/support/sysreq/files/python-compatibility.pdf\n",
    "https://www.mathworks.com/matlabcentral/answers/331186-running-matlab-python-interface-in-jupyter-notebook-throws-weird-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e180bf9-78df-4e28-ad58-f14199bd97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ff36ef-4b58-4872-87b5-fd15e0b10de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import KernelRegDraft as kreg\n",
    "start = timeit.timeit()\n",
    "# only use these clusters includes first 10 clusters in clusters_idx that pass quality\n",
    "Y, clusters_index = kreg.frequency_array(session = 'Theiler_2017-10-11', \n",
    "                                    bin_size = 0.005, \n",
    "                                    only_use_these_clusters=[ 3,  4, 7],\n",
    "                                    select_trials = np.array([4,5,6,7])\n",
    "                                    FILEPATH = path_to_data)\n",
    "end= timeit.timeit()\n",
    "print(start-end)\n",
    "\n",
    "pathforus = os.fspath(r'C:\\Users\\angus\\Desktop\\SteinmetzLab\\9598406\\spikeAndBehavioralData\\allData')\n",
    "\n",
    "trialstest = stein.calldata('Theiler_2017-10-11', \n",
    "                            ['trials.intervals.npy',\n",
    "                             'trials.included.npy'],\n",
    "                steinmetzpath=pathforus)\n",
    "\n",
    "#select_these = np.array([4,5,6,7])\n",
    "select_these = []\n",
    "\n",
    "if len(select_these)!=0:\n",
    "    trialsincludedtest = select_these\n",
    "elif True:   #filter by engagement\n",
    "    trialsincludedtest = trialstest['trialsincluded']\n",
    "\n",
    "[ i for i in range(0,len(trialsincluded)) if trialsincluded[i]]\n",
    "\n",
    "trialsintervalstest = trialstest['trialsintervals']\n",
    "trialsintervalstest = trialsintervalstest[trialsincludedtest,:]\n",
    "\n",
    "trialsintervalstest = trialstest['trialsintervals']\n",
    "trialsintervalstest = trialsintervalstest[trialsincludedtest.reshape(trialsintervalstest.shape),:]\n",
    "\n",
    "\n",
    "#again with more cl\"usters, \n",
    "\"\"\"\n",
    "Fixed the last error but now it's printing out the clusters for some weird reason'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "start = timeit.timeit()\n",
    "# only use these clusters includes first 10 clusters in clusters_idx that pass quality\n",
    "Y, clusters_index = kreg.frequency_array(session = 'Theiler_2017-10-11', \n",
    "                                    bin_size = 0.005, \n",
    "                                    only_use_these_clusters=[ 3,  4,  7,  9, 12, 14, 16, 17, 18, 19]\n",
    "                                    FILEPATH = path_to_data)\n",
    "end= timeit.timeit()\n",
    "print(start-end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c897a561-2972-4725-9a81-fce78b74fb68",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3417460924.py, line 92)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    in_bounds = (t =< len(interval_x) ) | (t =< len(interval_y) )\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Making the actual regression\n",
    "# CCA between P and Y to get b\n",
    "sklearn.cross_decomposition.CCA(n_components=2, *, \n",
    "                                scale=True, \n",
    "                                max_iter=500, \n",
    "                                tol=1e-06, \n",
    "                                copy=True)\n",
    "#Test\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "#could use make regression to simulate data\n",
    "#X, y = make_regression(n_features=2, random_state=0)\n",
    "\n",
    "Xtest = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]]\n",
    "Ytest = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n",
    "cca = CCA(n_components=2)\n",
    "cca.fit(Xtest, Ytest)\n",
    "CCA(n_components=2)\n",
    "X_c, Y_c = cca.transform(Xtest, Ytest)\n",
    "\n",
    "\n",
    "# run the regression\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "Ytest = np.array(Ytest)\n",
    "for n in range(0, Ytest.shape[0]):\n",
    "    print(n)\n",
    "    y = Ytest[n,:]\n",
    "    #X_c, y = make_regression(n_features=2, random_state=0)\n",
    "    regr = ElasticNetCV(cv=5, random_state=0)\n",
    "    regr.fit(X_c, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db472771-2b7b-45f2-b61e-197c01de60bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wintermute/anaconda3/envs/stein-env/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7d55084c-ae2f-499b-a7fa-869b2071d1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.007615499955136329\n"
     ]
    }
   ],
   "source": [
    "import KernelRegDraft as kreg\n",
    "start = timeit.timeit()\n",
    "# only use these clusters includes first 10 clusters in clusters_idx that pass quality\n",
    "Y, clusters_index = kreg.frequency_array(session = 'Theiler_2017-10-11', \n",
    "                                    bin_size = 0.005, \n",
    "                                    only_use_these_clusters=[ 3,  4, 7],\n",
    "                                    select_trials = np.array([4,5,6,7]),\n",
    "                                    FILEPATH = path_to_data)\n",
    "end= timeit.timeit()\n",
    "print(start-end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "697e2453-3528-4745-adf1-113a4bf21fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /home/wintermute/anaconda3\n",
      "brainpainterEnv          /home/wintermute/anaconda3/envs/brainpainterEnv\n",
      "stein-env             *  /home/wintermute/anaconda3/envs/stein-env\n",
      "                         /home/wintermute/anaconda_ete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47459da8-3921-4885-af27-81730404a22c",
   "metadata": {},
   "source": [
    "To Do:  Try out their one.api set up from version 3.  \n",
    "https://figshare.com/articles/dataset/Distributed_coding_of_choice_action_and_engagement_across_the_mouse_brain/9974357/3\n",
    "\n",
    "\"The data can be easily searched and loaded in Python using the ONE interface. The API may be installed via pip:\n",
    "\n",
    "pip install ONE-api\n",
    "\n",
    "Notes about the data format : https://github.com/nsteinme/steinmetz-et-al-2019/wiki/data-files\n",
    "\n",
    "To search and download this dataset:\n",
    "\n",
    "from one.api import One\n",
    "one = One(cache_dir='./9974357') # The location of the unarchived data\n",
    "sessions = one.search(dataset='trials') # search for all sessions that have a `trials` object\n",
    "session = sessions[0] # take the first session\n",
    "trials = one.load_object(session, 'trials') # load the trials object\n",
    "print(trials.intervals) # trials is a Bunch, values are NumPy arrays or pandas DataFrames\n",
    "print(trials.goCue_times)\n",
    "\n",
    "For further documentation, see https://int-brain-lab.github.io/iblenv/03_one.html\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "968412f1-301e-445f-8617-eb8941485052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from one.api import One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1d5a4111-485d-4acb-869b-c46477cf04c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = One(cache_dir= '../Steinmetz_et_al_2019_9974357/9974357')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4caf9f0a-c267-4ea1-888d-8ca57101de68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions = one.search(dataset='trials') # search for all sessions that have a `trials` object\n",
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0fb9d784-29af-40a2-a421-b8940b70d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mfig1_BehavAndRec.jpg\u001b[0m*  \u001b[34;42mspikeAndBehavioralData\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ba9ef629-3c0b-4280-a2dc-253d9aee0d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sessions[0] # take the first session\n",
    "trials = one.load_object(session, 'trials') # load the trials object\n",
    "print(trials.intervals) # trials is a Bunch, values are NumPy arrays or pandas DataFrames\n",
    "print(trials.goCue_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40e426-9cad-483a-9bcb-6a0cafcb5261",
   "metadata": {},
   "source": [
    "There are examples of using IBL data to generate psychometric curves.  https://github.com/int-brain-lab/paper-behavior\n",
    "Within ExploreIBLPIpelines there is a folder showing how to explore electrophysiology data.\n",
    "\n",
    "\n",
    "From here:  https://int-brain-lab.github.io/iblenv/one_docs/one_reference.html#alf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51eee0-b590-4bf3-96fc-35e4cf4f57fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
